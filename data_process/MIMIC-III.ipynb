{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa83b3-08b6-47bb-a13f-7590f6005a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c397857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "from heterogt.utils.seed import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d69f534-75bf-4a44-ad02-cf604130d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected Apple MPS backend (Mac with M1/M2/M3). Skipping CUDA seeds.\n",
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdc27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cb16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./MIMIC-III-raw/\"\n",
    "# 'HADM_ID': encounter id\n",
    "# 'SUBJECT_ID': patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48eb4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_file = os.path.join(data_path, \"PRESCRIPTIONS.csv.gz\")\n",
    "procedure_file = os.path.join(data_path, \"PROCEDURES_ICD.csv.gz\")\n",
    "diag_file = os.path.join(data_path, \"DIAGNOSES_ICD.csv.gz\")\n",
    "admission_file = os.path.join(data_path, \"ADMISSIONS.csv.gz\")\n",
    "lab_test_file = os.path.join(data_path, \"LABEVENTS.csv.gz\")\n",
    "patient_file = os.path.join(data_path, \"PATIENTS.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8183d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug code mapping files from GAMENet repo\n",
    "# https://github.com/sjy1203/GAMENet/tree/master/data\n",
    "ndc2atc_file = './GAMENet/ndc2atc_level4.csv' \n",
    "cid_atc = './GAMENet/drug-atc.csv'\n",
    "ndc2rxnorm_file = './GAMENet/ndc2rxnorm_mapping.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671a92a",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d352ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_med():\n",
    "    med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n",
    "    # filter\n",
    "    med_pd.drop(columns=['ROW_ID','DRUG_TYPE','DRUG_NAME_POE','DRUG_NAME_GENERIC',\n",
    "                     'FORMULARY_DRUG_CD','GSN','PROD_STRENGTH','DOSE_VAL_RX',\n",
    "                     'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP','FORM_UNIT_DISP',\n",
    "                      'ROUTE','ENDDATE','DRUG'], axis=1, inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['NDC'] == '0'].index, axis=0, inplace=True)\n",
    "    med_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE'], inplace=True)\n",
    "    med_pd.fillna(method='pad', inplace=True)\n",
    "    med_pd.dropna(inplace=True)\n",
    "    med_pd.drop_duplicates(inplace=True)\n",
    "    med_pd['ICUSTAY_ID'] = med_pd['ICUSTAY_ID'].astype('int64')\n",
    "    med_pd['STARTDATE'] = pd.to_datetime(med_pd['STARTDATE'], format='%Y-%m-%d %H:%M:%S')    \n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    \n",
    "    def filter_first24hour_med(med_pd):\n",
    "        med_pd_new = med_pd.drop(columns=['NDC'])\n",
    "        med_pd_new = med_pd_new.groupby(by=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).head(1).reset_index(drop=True)\n",
    "        med_pd_new = pd.merge(med_pd_new, med_pd, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','STARTDATE'])\n",
    "        med_pd_new = med_pd_new.drop(columns=['STARTDATE'])\n",
    "        return med_pd_new\n",
    "    \n",
    "    med_pd = filter_first24hour_med(med_pd) \n",
    "    \n",
    "    med_pd = med_pd.drop(columns=['ICUSTAY_ID'])\n",
    "    med_pd = med_pd.drop_duplicates() \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_procedure():\n",
    "    pro_pd = pd.read_csv(procedure_file, dtype={'ICD9_CODE':'category'})\n",
    "    pro_pd.drop(columns=['ROW_ID'], inplace=True)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop(columns=['SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.reset_index(drop=True, inplace=True)\n",
    "    pro_pd['ICD9_CODE'] = 'PRO_' + pro_pd['ICD9_CODE'].astype(str)\n",
    "    return pro_pd\n",
    "\n",
    "def process_diag():\n",
    "    diag_pd = pd.read_csv(diag_file)\n",
    "    diag_pd.dropna(inplace=True)\n",
    "    diag_pd.drop(columns=['SEQ_NUM','ROW_ID'],inplace=True)\n",
    "    diag_pd.drop_duplicates(inplace=True)\n",
    "    diag_pd.sort_values(by=['SUBJECT_ID','HADM_ID'], inplace=True)\n",
    "    diag_pd[\"ICD9_CODE\"] = \"DIAG_\" + diag_pd[\"ICD9_CODE\"].astype(str)\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def process_admission():\n",
    "    ad_pd = pd.read_csv(admission_file)\n",
    "    patient_pd = pd.read_csv(patient_file)\n",
    "    \n",
    "    ad_pd.drop(columns=['ROW_ID', 'ADMISSION_LOCATION',\n",
    "       'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION',\n",
    "       'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS',\n",
    "       'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA'], axis=1, inplace=True)\n",
    "    \n",
    "    patient_pd.drop(columns=['ROW_ID','DOD','DOD_HOSP','DOD_SSN',\"EXPIRE_FLAG\"], axis=1, inplace=True)\n",
    "    \n",
    "    ad_pd[\"ADMITTIME\"] = pd.to_datetime(ad_pd['ADMITTIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    ad_pd[\"DISCHTIME\"] = pd.to_datetime(ad_pd['DISCHTIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')  # time for leaving hospital\n",
    "    ad_pd[\"STAY_DAYS\"] = (ad_pd[\"DISCHTIME\"] - ad_pd[\"ADMITTIME\"]).dt.days\n",
    "    patient_pd['DOB'] = pd.to_datetime(patient_pd['DOB'], format='%Y-%m-%d %H:%M:%S', errors='coerce')  # birthday\n",
    "    \n",
    "    ad_pd = ad_pd.merge(patient_pd, on=['SUBJECT_ID'], how='inner')\n",
    "    \n",
    "    # create features: age, death, number of days in this encounter, readmission (next visit)\n",
    "    ad_pd[\"AGE\"] = ad_pd['ADMITTIME'].dt.year - ad_pd['DOB'].dt.year\n",
    "    ad_pd[ad_pd[\"AGE\"] >= 300] = 90\n",
    "    age = ad_pd[\"AGE\"]\n",
    "    bins = np.linspace(age.min(), age.max(), 20 + 1)\n",
    "    ad_pd['AGE'] = pd.cut(age, bins=bins, labels=False, include_lowest=True)\n",
    "    ad_pd['AGE'] = \"AGE_\" + ad_pd[\"AGE\"].astype(\"str\")\n",
    "    \n",
    "    ad_pd[\"DEATH\"] = ad_pd[\"DEATHTIME\"].notna()\n",
    "\n",
    "    ad_pd['ADMITTIME'] = ad_pd['ADMITTIME'].astype(str)\n",
    "    ad_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ADMITTIME'], inplace=True)\n",
    "    ad_pd['next_visit'] = ad_pd.groupby('SUBJECT_ID')['HADM_ID'].shift(-1)\n",
    "    ad_pd['READMISSION'] = ad_pd['next_visit'].notnull().astype(int)\n",
    "    ad_pd.drop('next_visit', axis=1, inplace=True)\n",
    "\n",
    "    ad_pd.drop(columns=['DISCHTIME', 'DOB', 'DEATHTIME'], axis=1, inplace=True)\n",
    "    ad_pd.drop_duplicates(inplace=True)\n",
    "    return ad_pd.reset_index(drop=True)\n",
    "\n",
    "def process_lab_test(n_bins=5):\n",
    "    lab_pd = pd.read_csv(lab_test_file)\n",
    "    lab_pd = lab_pd.groupby(by=['SUBJECT_ID','ITEMID']).head(1).reset_index(drop=True)  # only consider the first value\n",
    "    lab_pd = lab_pd[lab_pd[\"VALUENUM\"].notna()]\n",
    "    lab_pd = lab_pd[lab_pd[\"HADM_ID\"].notna()]\n",
    "    \n",
    "    lab_pd.drop(columns=['ROW_ID'], axis=1, inplace=True)\n",
    "    \n",
    "    def contains_text(group):\n",
    "        for item in group:\n",
    "            try:\n",
    "                float(item)\n",
    "            except ValueError:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for itemid in lab_pd['ITEMID'].unique():\n",
    "        group = lab_pd[lab_pd['ITEMID'] == itemid]['VALUE']\n",
    "\n",
    "        # if the lab test contains text value then directly copy the value\n",
    "        if contains_text(group):\n",
    "            lab_pd.loc[lab_pd['ITEMID'] == itemid, 'value_bin'] = group\n",
    "        else:\n",
    "            # value->numeric\n",
    "            values_numeric = pd.to_numeric(group, errors='coerce')\n",
    "\n",
    "            if len(values_numeric.dropna()) < n_bins:\n",
    "                lab_pd.loc[lab_pd['ITEMID'] == itemid, 'value_bin'] = group\n",
    "            else:\n",
    "                lab_pd.loc[lab_pd['ITEMID'] == itemid, 'value_bin'] = pd.qcut(values_numeric, q=n_bins, labels=False, duplicates='drop')\n",
    "        \n",
    "    lab_pd[\"ITEMID\"] = lab_pd[\"ITEMID\"].astype(str)\n",
    "    lab_pd[\"value_bin\"] = lab_pd[\"value_bin\"].astype(str)\n",
    "    lab_pd[\"LAB_TEST\"] = lab_pd[[\"ITEMID\", \"value_bin\"]].apply(\"-\".join, axis=1)\n",
    "    \n",
    "    lab_pd.drop(columns=['CHARTTIME', 'VALUE', 'VALUENUM', 'VALUEUOM', 'FLAG', 'value_bin', 'ITEMID'], axis=1, inplace=True)\n",
    "    lab_pd.drop_duplicates(inplace=True)\n",
    "    lab_pd.reset_index(drop=True, inplace=True)\n",
    "    lab_pd['LAB_TEST'] = \"LAB_\" + lab_pd[\"LAB_TEST\"].astype(str)\n",
    "    return lab_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2642a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndc2atc4(med_pd):\n",
    "    with open(ndc2rxnorm_file, 'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    med_pd['RXCUI'] = med_pd['NDC'].map(ndc2rxnorm)\n",
    "    med_pd.dropna(inplace=True)\n",
    "\n",
    "    rxnorm2atc = pd.read_csv(ndc2atc_file)\n",
    "    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','NDC'])\n",
    "    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
    "    \n",
    "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n",
    "    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n",
    "    med_pd = med_pd.rename(columns={'ATC4':'NDC'})\n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: x[:4])\n",
    "    med_pd = med_pd.drop_duplicates()\n",
    "    med_pd[\"NDC\"] = \"MED_\" + med_pd[\"NDC\"].astype(str)    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_pro(pro_pd, threshold=800):\n",
    "    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:threshold, 'ICD9_CODE'])]\n",
    "    return pro_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_diag(diag_pd, threshold=2000):\n",
    "    diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:threshold, 'ICD9_CODE'])]\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_lab(lab_pd, threshold=1500):\n",
    "    lab_count = lab_pd.groupby(by=['LAB_TEST']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    lab_pd = lab_pd[lab_pd['LAB_TEST'].isin(lab_count.loc[:threshold, 'LAB_TEST'])]\n",
    "    return lab_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00cbbfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_visit_patients(df: pd.DataFrame, visit_threshold: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all rows corresponding to patients with more than `visit_threshold` unique visits.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input EHR dataframe. Must contain 'SUBJECT_ID' and 'HADM_ID'.\n",
    "        visit_threshold (int): The maximum number of visits allowed per patient.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered dataframe containing only patients with ≤ visit_threshold visits.\n",
    "    \"\"\"\n",
    "    # Step 1: Count visits per patient\n",
    "    visit_counts = df.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique()\n",
    "\n",
    "    # Step 2: Identify valid SUBJECT_IDs\n",
    "    valid_subjects = visit_counts[visit_counts <= visit_threshold].index\n",
    "\n",
    "    # Step 3: Filter the dataframe\n",
    "    filtered_df = df[df[\"SUBJECT_ID\"].isin(valid_subjects)].copy()\n",
    "\n",
    "    # Optional: logging\n",
    "    num_removed_patients = df[\"SUBJECT_ID\"].nunique() - filtered_df[\"SUBJECT_ID\"].nunique()\n",
    "    num_removed_rows = len(df) - len(filtered_df)\n",
    "    print(f\"Removed {num_removed_patients} patients and {num_removed_rows} rows exceeding {visit_threshold} visits.\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28edc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process_all():\n",
    "    print('process_med')\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "\n",
    "    print('process_diag')\n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_most_diag(diag_pd)\n",
    "\n",
    "    print('process_pro')\n",
    "    pro_pd = process_procedure()\n",
    "    pro_pd = filter_most_pro(pro_pd)\n",
    "\n",
    "    print('process_ad')\n",
    "    ad_pd = process_admission()\n",
    "    \n",
    "    print('process_lab')\n",
    "    lab_pd = process_lab_test()\n",
    "    lab_pd = filter_most_lab(lab_pd)\n",
    "    \n",
    "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    lab_pd_key = lab_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    ad_pd_key = ad_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    \n",
    "    # filter key\n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(lab_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(ad_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    lab_pd = lab_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    ad_pd = ad_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
    "    lab_pd = lab_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['LAB_TEST'].unique().reset_index()\n",
    "    \n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n",
    "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
    "    lab_pd['LAB_TEST'] = lab_pd['LAB_TEST'].map(lambda x: list(x))\n",
    "    \n",
    "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(lab_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(ad_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    data = data.sort_values(by=['SUBJECT_ID', 'ADMITTIME'])\n",
    "    \n",
    "    # filter rows without disease codes\n",
    "    data = data[data['ICD9_CODE'].notnull()]\n",
    "\n",
    "    # create feature: readmission within 30/90 days\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])\n",
    "    data['READMISSION_1M'] = data.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1) - data['ADMITTIME']\n",
    "    data['READMISSION_3M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=90) else 0)\n",
    "    data['READMISSION_1M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=30) else 0)\n",
    "    \n",
    "    # create feature: diease in next 6/12 months    \n",
    "    data['NEXT_DIAG_6M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                              (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                              (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=180))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_12M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                                   (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                                   (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=365))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_6M'] = data['NEXT_DIAG_6M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    data['NEXT_DIAG_12M'] = data['NEXT_DIAG_12M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    \n",
    "    data.drop(columns=['ADMITTIME'], axis=1, inplace=True)\n",
    "    data = remove_high_visit_patients(data, visit_threshold=8)\n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ce32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(data):\n",
    "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
    "    print('#clinical events ', len(data))\n",
    "    \n",
    "    diag = data['ICD9_CODE'].values\n",
    "    med = data['NDC'].values\n",
    "    pro = data['PRO_CODE'].values\n",
    "    lab_test = data['LAB_TEST'].values\n",
    "    \n",
    "    unique_diag = set([j for i in diag for j in list(i)])\n",
    "    unique_med = set([j for i in med for j in list(i)])\n",
    "    unique_pro = set([j for i in pro for j in list(i)])\n",
    "    unique_lab = set([j for i in lab_test for j in list(i)])\n",
    "    \n",
    "    print('#diagnosis ', len(unique_diag))\n",
    "    print('#med ', len(unique_med))\n",
    "    print('#procedure', len(unique_pro))\n",
    "    print('#lab', len(unique_lab))\n",
    "    \n",
    "    avg_diag = avg_med = avg_pro = avg_lab = 0\n",
    "    max_diag = max_med = max_pro = max_lab = 0\n",
    "    cnt = max_visit = avg_visit = 0\n",
    "\n",
    "    for subject_id in data['SUBJECT_ID'].unique():\n",
    "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
    "        x, y, z, k = [], [], [], []\n",
    "        visit_cnt = 0\n",
    "        for index, row in item_data.iterrows():\n",
    "            visit_cnt += 1\n",
    "            cnt += 1\n",
    "            x.extend(list(row['ICD9_CODE']))\n",
    "            y.extend(list(row['NDC']))\n",
    "            z.extend(list(row['PRO_CODE']))\n",
    "            k.extend(list(row['LAB_TEST']))\n",
    "        x, y, z, k = set(x), set(y), set(z), set(k)\n",
    "        avg_diag += len(x)\n",
    "        avg_med += len(y)\n",
    "        avg_pro += len(z)\n",
    "        avg_lab += len(k)\n",
    "        avg_visit += visit_cnt\n",
    "        if len(x) > max_diag:\n",
    "            max_diag = len(x)\n",
    "        if len(y) > max_med:\n",
    "            max_med = len(y) \n",
    "        if len(z) > max_pro:\n",
    "            max_pro = len(z)\n",
    "        if len(k) > max_lab:\n",
    "            max_lab = len(k)\n",
    "        if visit_cnt > max_visit:\n",
    "            max_visit = visit_cnt\n",
    "\n",
    "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
    "    print('#avg of medicines ', avg_med/ cnt)\n",
    "    print('#avg of procedures ', avg_pro/ cnt)\n",
    "    print('#avg of lab_test ', avg_lab/ cnt)\n",
    "    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
    "\n",
    "    print('#max of diagnoses ', max_diag)\n",
    "    print('#max of medicines ', max_med)\n",
    "    print('#max of procedures ', max_pro)\n",
    "    print('#max of lab_test ', max_lab)\n",
    "    print('#max of visit ', max_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2238e318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n",
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  med_pd.fillna(method='pad', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_diag\n",
      "process_pro\n",
      "process_ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:70: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '90' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  ad_pd[ad_pd[\"AGE\"] >= 300] = 90\n",
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:70: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '90' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  ad_pd[ad_pd[\"AGE\"] >= 300] = 90\n",
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:70: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '90' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  ad_pd[ad_pd[\"AGE\"] >= 300] = 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_lab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2281/3090984958.py:115: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['107' '5130' '216']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  lab_pd.loc[lab_pd['ITEMID'] == itemid, 'value_bin'] = group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 20 patients and 212 rows exceeding 8 visits.\n"
     ]
    }
   ],
   "source": [
    "data = process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0042a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#patients  (33067,)\n",
      "#clinical events  39976\n",
      "#diagnosis  1998\n",
      "#med  145\n",
      "#procedure 801\n",
      "#lab 1500\n",
      "#avg of diagnoses  10.831999199519712\n",
      "#avg of medicines  7.820442265359215\n",
      "#avg of procedures  4.481113668200921\n",
      "#avg of lab_test  41.873649189513706\n",
      "#avg of vists  1.2089394260138506\n",
      "#max of diagnoses  105\n",
      "#max of medicines  50\n",
      "#max of procedures  48\n",
      "#max of lab_test  169\n",
      "#max of visit  8\n"
     ]
    }
   ],
   "source": [
    "statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c5c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MIMIC-III-processed/mimic.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01731cb4",
   "metadata": {},
   "source": [
    "# Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ef2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data = pickle.load(open(\"./MIMIC-III-processed/mimic.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a331d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33067"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_data[\"SUBJECT_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4194b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n",
      "2904\n",
      "3438\n",
      "4367\n"
     ]
    }
   ],
   "source": [
    "# get the patient id with the labels\n",
    "pat_readmission = set(mimic_data[mimic_data[\"READMISSION_1M\"] == 1][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_readmission))\n",
    "pat_nextdiag_6m = set(mimic_data[mimic_data[\"NEXT_DIAG_6M\"].notna()][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_nextdiag_6m))\n",
    "# note that here we extract at the patient level, not the encounter level\n",
    "# so patients with at least one encounter with a next diagnosis in 6 / 12 months will still have\n",
    "# some encounters without a next diagnosis. This part is addressed in the HBERTFinetuneEHRDataset.\n",
    "pat_nextdiag_12m = set(mimic_data[mimic_data[\"NEXT_DIAG_12M\"].notna()][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_nextdiag_12m))\n",
    "pat_death = set(mimic_data[mimic_data[\"DEATH\"]][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_death))\n",
    "pat_all_label = list(pat_readmission | pat_nextdiag_6m | pat_nextdiag_12m | pat_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34f85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_all = mimic_data[\"SUBJECT_ID\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0bd0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23146 9921\n"
     ]
    }
   ],
   "source": [
    "n_pretrain_patient = int(len(pat_all) * 0.7)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pretrain_patient = np.random.choice(list(set(pat_all) - set(pat_all_label)), n_pretrain_patient, replace=False).tolist()\n",
    "downstream_patient = list(set(pat_all) - set(pretrain_patient))\n",
    "print(len(pretrain_patient), len(downstream_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dea4a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrain_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(pretrain_patient))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1275dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream task: PLOS, death prediction, readmission\n",
    "train_ratio, val_ratio = 0.2, 0.4\n",
    "n_finetune_pat, n_val_pat = int(len(downstream_patient) * train_ratio), int(len(downstream_patient) * val_ratio)\n",
    "downstream_patient = sorted(downstream_patient) \n",
    "np.random.seed(RANDOM_SEED + 1)\n",
    "np.random.shuffle(downstream_patient)\n",
    "finetune_pat, val_pat, test_pat = downstream_patient[:n_finetune_pat], \\\n",
    "                                    downstream_patient[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    downstream_patient[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream task: next diagnosis prediction\n",
    "# only use the patient that has multiple visits\n",
    "# 6m\n",
    "train_ratio, val_ratio = 0.4, 0.3\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_6m) * train_ratio), int(len(pat_nextdiag_6m) * val_ratio)\n",
    "pat_nextdiag_6m = list(pat_nextdiag_6m)\n",
    "pat_nextdiag_6m = sorted(list(pat_nextdiag_6m)) \n",
    "np.random.seed(RANDOM_SEED + 2) \n",
    "np.random.shuffle(pat_nextdiag_6m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_6m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]\n",
    "# 12m\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_12m) * train_ratio), int(len(pat_nextdiag_12m) * val_ratio)\n",
    "pat_nextdiag_12m = list(pat_nextdiag_12m)\n",
    "pat_nextdiag_12m = sorted(list(pat_nextdiag_12m))\n",
    "np.random.seed(RANDOM_SEED + 3) \n",
    "np.random.shuffle(pat_nextdiag_12m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_12m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b76e9935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./MIMIC-III-processed/mimic_pretrain.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(pretrain_dataset, outfile)\n",
    "with open(\"./MIMIC-III-processed/mimic_downstream.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset, val_dataset, test_dataset], outfile)\n",
    "with open(\"./MIMIC-III-processed/mimic_nextdiag_6m.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset6m, val_dataset6m, test_dataset6m], outfile)\n",
    "with open(\"./MIMIC-III-processed/mimic_nextdiag_12m.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset12m, val_dataset12m, test_dataset12m], outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
