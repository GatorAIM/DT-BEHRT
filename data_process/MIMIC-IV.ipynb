{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f133fc2-049b-46f9-9ce7-7bd41ad1274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c397857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "from heterogt.utils.seed import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e970d2-4f08-432c-854c-522ed1fa2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected Apple MPS backend (Mac with M1/M2/M3). Skipping CUDA seeds.\n",
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdc27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from icdmappings import Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cb16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./MIMIC-IV-raw/\"\n",
    "# 'HADM_ID': encounter id\n",
    "# 'SUBJECT_ID': patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48eb4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_file = os.path.join(data_path, \"prescriptions.csv.gz\")\n",
    "procedure_file = os.path.join(data_path, \"procedures_icd.csv.gz\")\n",
    "diag_file = os.path.join(data_path, \"diagnoses_icd.csv.gz\")\n",
    "admission_file = os.path.join(data_path, \"admissions.csv.gz\")\n",
    "lab_test_file = os.path.join(data_path, \"labevents.csv.gz\")\n",
    "patient_file = os.path.join(data_path, \"patients.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8183d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug code mapping files from GAMENet repo\n",
    "# https://github.com/sjy1203/GAMENet/tree/master/data\n",
    "ndc2atc_file = './GAMENet/ndc2atc_level4.csv' \n",
    "cid_atc = './GAMENet/drug-atc.csv'\n",
    "ndc2rxnorm_file = './GAMENet/ndc2rxnorm_mapping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebfd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_med():\n",
    "    med_pd = pd.read_csv(med_file, dtype={'ndc':'category'})\n",
    "    med_pd.drop(columns=['pharmacy_id','poe_id','poe_seq','order_provider_id',\n",
    "                     'stoptime','drug_type','drug','formulary_drug_cd',\n",
    "                     'gsn','prod_strength','form_rx','dose_val_rx',\n",
    "                      'dose_unit_rx','form_val_disp','form_unit_disp', \n",
    "                      'doses_per_24_hrs', 'route'], axis=1, inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['ndc'] == '0'].index, axis=0, inplace=True)\n",
    "    med_pd.sort_values(by=['subject_id', 'hadm_id', 'starttime'], inplace=True)\n",
    "    med_pd.fillna(method='pad', inplace=True)\n",
    "    med_pd.dropna(inplace=True)\n",
    "    med_pd.drop_duplicates(inplace=True)\n",
    "    med_pd['starttime'] = pd.to_datetime(med_pd['starttime'], format='%Y-%m-%d %H:%M:%S')    \n",
    "    med_pd = med_pd.reset_index(drop=True)  \n",
    "    \n",
    "    def filter_first24hour_med(med_pd):\n",
    "        med_pd_new = med_pd.drop(columns=['ndc'])\n",
    "        med_pd_new = med_pd_new.groupby(by=['subject_id','hadm_id']).head(1).reset_index(drop=True)\n",
    "        med_pd_new = pd.merge(med_pd_new, med_pd, on=['subject_id','hadm_id', 'starttime'])\n",
    "        med_pd_new = med_pd_new.drop(columns=['starttime'])\n",
    "        return med_pd_new\n",
    "        \n",
    "    med_pd = filter_first24hour_med(med_pd) \n",
    "    med_pd = med_pd.drop_duplicates() \n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd.rename(columns={'subject_id': 'SUBJECT_ID', 'hadm_id': 'HADM_ID', 'ndc': 'NDC'}, inplace=True)\n",
    "    return med_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb2ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_icd_code(pd):\n",
    "    mapper = Mapper()\n",
    "    mask_icd10 = pd['icd_version'] == 10\n",
    "    icd10_codes = pd.loc[mask_icd10, 'icd_code']\n",
    "    # Vectorized mapping: returns None if mapping fails\n",
    "    def safe_map(x):\n",
    "        try:\n",
    "            mapped = mapper.map(x, source='icd10', target='icd9')\n",
    "            return mapped if mapped else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    icd9_mapped = icd10_codes.apply(safe_map)\n",
    "    pd.loc[mask_icd10, 'icd_code'] = icd9_mapped\n",
    "    pd.dropna(subset=['icd_code'], inplace=True)\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82d5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_procedure():\n",
    "    pro_pd = pd.read_csv(procedure_file, dtype={'icd_code': 'str'})\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.sort_values(by=['subject_id', 'hadm_id', 'seq_num'], inplace=True)\n",
    "    pro_pd.drop(columns=['seq_num', 'chartdate'], errors='ignore', inplace=True)\n",
    "\n",
    "    # Map ICD-10 to ICD-9\n",
    "    pro_pd = convert_icd_code(pro_pd)\n",
    "\n",
    "    # Remove duplicates and reset index\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert to category if needed\n",
    "    pro_pd['icd_code'] = pro_pd['icd_code'].astype('category')\n",
    "    pro_pd.drop(columns=['icd_version'], errors='ignore', inplace=True)\n",
    "    pro_pd.rename(columns={'subject_id': 'SUBJECT_ID', 'hadm_id': 'HADM_ID', 'icd_code': 'ICD9_CODE'}, inplace=True)\n",
    "    pro_pd['ICD9_CODE'] = 'PRO_' + pro_pd['ICD9_CODE'].astype(str)\n",
    "    return pro_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbfbaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diag():\n",
    "    diag_pd = pd.read_csv(diag_file)\n",
    "    diag_pd.dropna(inplace=True)\n",
    "    diag_pd.sort_values(by=['subject_id', 'hadm_id', 'seq_num'], inplace=True)\n",
    "    diag_pd.drop(columns=['seq_num'], inplace=True)\n",
    "    diag_pd = convert_icd_code(diag_pd)\n",
    "    diag_pd.drop_duplicates(inplace=True)\n",
    "    diag_pd = diag_pd.reset_index(drop=True)\n",
    "    diag_pd.drop(columns=['icd_version'], errors='ignore', inplace=True)\n",
    "    diag_pd.rename(columns={'subject_id': 'SUBJECT_ID', 'hadm_id': 'HADM_ID', 'icd_code': 'ICD9_CODE'}, inplace=True)\n",
    "    diag_pd[\"ICD9_CODE\"] = \"DIAG_\" + diag_pd[\"ICD9_CODE\"].astype(str)\n",
    "    return diag_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7a3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_admission():\n",
    "    ad_pd = pd.read_csv(admission_file)\n",
    "    patient_pd = pd.read_csv(patient_file)\n",
    "    ad_pd.drop(columns=['admit_provider_id', 'admission_location',\n",
    "       'discharge_location', 'insurance', 'language', 'marital_status',\n",
    "       'race', 'edregtime', 'edouttime', 'hospital_expire_flag'], axis=1, inplace=True)\n",
    "    patient_pd.drop(columns=['anchor_year','anchor_year_group','dod'], axis=1, inplace=True)\n",
    "    ad_pd[\"admittime\"] = pd.to_datetime(ad_pd['admittime'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    ad_pd[\"dischtime\"] = pd.to_datetime(ad_pd['dischtime'], format='%Y-%m-%d %H:%M:%S', errors='coerce')  # time for leaving hospital\n",
    "    ad_pd[\"deathtime\"] = pd.to_datetime(ad_pd['deathtime'], format='%Y-%m-%d %H:%M:%S', errors='coerce')  # time for leaving hospital\n",
    "    ad_pd[\"STAY_DAYS\"] = (ad_pd[\"dischtime\"] - ad_pd[\"admittime\"]).dt.days\n",
    "    ad_pd = ad_pd.merge(patient_pd, on=['subject_id'], how='inner')\n",
    "    ad_pd.loc[ad_pd[\"anchor_age\"] >= 300, \"anchor_age\"] = 90\n",
    "    age = ad_pd[\"anchor_age\"]\n",
    "    bins = np.linspace(age.min(), age.max(), 20 + 1)\n",
    "    ad_pd['anchor_age'] = pd.cut(age, bins=bins, labels=False, include_lowest=True)\n",
    "    ad_pd['anchor_age'] = \"AGE_\" + ad_pd[\"anchor_age\"].astype(\"str\")\n",
    "    ad_pd[\"DEATH\"] = ad_pd[\"deathtime\"].notna()\n",
    "    ad_pd['admittime'] = ad_pd['admittime'].astype(str)\n",
    "    ad_pd.sort_values(by=['subject_id', 'hadm_id', 'admittime'], inplace=True)\n",
    "    ad_pd['next_visit'] = ad_pd.groupby('subject_id')['hadm_id'].shift(-1)\n",
    "    ad_pd['READMISSION'] = ad_pd['next_visit'].notnull().astype(int)\n",
    "    ad_pd.drop('next_visit', axis=1, inplace=True)\n",
    "    ad_pd.drop(columns=['dischtime', 'deathtime'], axis=1, inplace=True)\n",
    "    ad_pd.drop_duplicates(inplace=True)\n",
    "    ad_pd.reset_index(drop=True, inplace=True)\n",
    "    ad_pd.rename(columns={'subject_id': 'SUBJECT_ID', 'hadm_id': 'HADM_ID', 'admittime': 'ADMITTIME',\n",
    "                          'gender': 'GENDER', 'anchor_age': 'AGE', 'admission_type': 'ADMISSION_TYPE'}, inplace=True)\n",
    "    return ad_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35dbf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lab_test(n_bins=5):\n",
    "    lab_pd = pd.read_csv(lab_test_file)\n",
    "    # 每个(subject_id, itemid)只取第一次\n",
    "    lab_pd = lab_pd.groupby(by=['subject_id','itemid']).head(1).reset_index(drop=True)\n",
    "    # 仅保留有数值和有住院ID的记录（否则无法对 '___' 行用 valuenum）\n",
    "    lab_pd = lab_pd[lab_pd[\"valuenum\"].notna()]\n",
    "    lab_pd = lab_pd[lab_pd[\"hadm_id\"].notna()]\n",
    "    # 某些版本可能无此列，安全起见用 errors='ignore'\n",
    "    lab_pd.drop(columns=['labevent_id'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # 主循环：按 itemid 批处理\n",
    "    for itemid in lab_pd['itemid'].unique():\n",
    "        group_idx = (lab_pd['itemid'] == itemid)\n",
    "\n",
    "        group_value = lab_pd.loc[group_idx, 'value'].astype(str)\n",
    "        group_num_vn = pd.to_numeric(lab_pd.loc[group_idx, 'valuenum'], errors='coerce')  # 来自 valuenum 列\n",
    "        group_num_val = pd.to_numeric(group_value, errors='coerce')                       # 来自 value 列的“数值字符串”\n",
    "\n",
    "        # 三类掩码（互斥并覆盖全体）\n",
    "        mask_underscore = (group_value == '___')                                  # 需用 valuenum\n",
    "        mask_text_nonnum = group_value.notna() & (group_num_val.isna()) & (~mask_underscore)  # 非'___'文本\n",
    "        mask_numeric_in_value = group_num_val.notna() & (~mask_underscore)        # value 为数值字符串\n",
    "\n",
    "        # 初始化：先把结果设为原 value（确保“非'___'文本”能保留）\n",
    "        lab_pd.loc[group_idx, 'value_bin'] = group_value\n",
    "\n",
    "        # A) 处理 '___'：使用 valuenum（对属于 '___' 的子集分箱）\n",
    "        if mask_underscore.any():\n",
    "            vn_subset = group_num_vn[mask_underscore]\n",
    "            if vn_subset.notna().sum() >= n_bins:\n",
    "                lab_pd.loc[group_idx & mask_underscore, 'value_bin'] = pd.qcut(\n",
    "                    vn_subset, q=n_bins, labels=False, duplicates='drop'\n",
    "                ).astype('Int64').astype(str)\n",
    "            else:\n",
    "                # 仍然“使用 valuenum”，直接写回数值字符串\n",
    "                lab_pd.loc[group_idx & mask_underscore, 'value_bin'] = vn_subset.astype('Float64').astype(str)\n",
    "\n",
    "        # B) 处理数值字符串：仅对这些行分箱，保留其他行为原值\n",
    "        if mask_numeric_in_value.any():\n",
    "            val_subset = group_num_val[mask_numeric_in_value]\n",
    "            if val_subset.notna().sum() >= n_bins:\n",
    "                lab_pd.loc[group_idx & mask_numeric_in_value, 'value_bin'] = pd.qcut(\n",
    "                    val_subset, q=n_bins, labels=False, duplicates='drop'\n",
    "                ).astype('Int64').astype(str)\n",
    "            else:\n",
    "                # 数量不足：保留原 value（已在初始化时完成），无需额外操作\n",
    "                pass\n",
    "\n",
    "        # C) 非'___'文本行 mask_text_nonnum：已在初始化中保留，无需额外处理\n",
    "\n",
    "    # 统一与收尾\n",
    "    lab_pd[\"itemid\"] = lab_pd[\"itemid\"].astype(str)\n",
    "    lab_pd[\"value_bin\"] = lab_pd[\"value_bin\"].astype(str)\n",
    "    lab_pd[\"LAB_TEST\"] = lab_pd[[\"itemid\", \"value_bin\"]].apply(\"-\".join, axis=1)\n",
    "\n",
    "    lab_pd.drop(columns=[\n",
    "        'specimen_id', 'order_provider_id', 'itemid', 'charttime', 'storetime', 'value', 'valuenum', 'valueuom',\n",
    "        'ref_range_lower', 'ref_range_upper', 'flag', 'priority', 'comments', 'value_bin'\n",
    "    ], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    lab_pd.drop_duplicates(inplace=True)\n",
    "    lab_pd.reset_index(drop=True, inplace=True)\n",
    "    lab_pd.rename(columns={'subject_id': 'SUBJECT_ID', 'hadm_id': 'HADM_ID'}, inplace=True)\n",
    "    lab_pd['HADM_ID'] = lab_pd['HADM_ID'].astype('int64')\n",
    "    lab_pd['LAB_TEST'] = \"LAB_\" + lab_pd[\"LAB_TEST\"].astype(str)\n",
    "    return lab_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e044610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndc2atc4(med_pd):\n",
    "    with open(ndc2rxnorm_file, 'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    med_pd['RXCUI'] = med_pd['NDC'].map(ndc2rxnorm)\n",
    "    med_pd.dropna(inplace=True)\n",
    "\n",
    "    rxnorm2atc = pd.read_csv(ndc2atc_file)\n",
    "    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','NDC'])\n",
    "    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
    "    \n",
    "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n",
    "    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n",
    "    med_pd = med_pd.rename(columns={'ATC4':'NDC'})\n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: x[:4])\n",
    "    med_pd = med_pd.drop_duplicates()\n",
    "    med_pd[\"NDC\"] = \"MED_\" + med_pd[\"NDC\"].astype(str)    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_pro(pro_pd, threshold=800):\n",
    "    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:threshold, 'ICD9_CODE'])]\n",
    "    return pro_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_diag(diag_pd, threshold=2000):\n",
    "    diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:threshold, 'ICD9_CODE'])]\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_lab(lab_pd, threshold=1500):\n",
    "    lab_count = lab_pd.groupby(by=['LAB_TEST']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    lab_pd = lab_pd[lab_pd['LAB_TEST'].isin(lab_count.loc[:threshold, 'LAB_TEST'])]\n",
    "    return lab_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbe0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_visit_patients(df: pd.DataFrame, visit_threshold: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all rows corresponding to patients with more than `visit_threshold` unique visits.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input EHR dataframe. Must contain 'SUBJECT_ID' and 'HADM_ID'.\n",
    "        visit_threshold (int): The maximum number of visits allowed per patient.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered dataframe containing only patients with ≤ visit_threshold visits.\n",
    "    \"\"\"\n",
    "    # Step 1: Count visits per patient\n",
    "    visit_counts = df.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique()\n",
    "\n",
    "    # Step 2: Identify valid SUBJECT_IDs\n",
    "    valid_subjects = visit_counts[visit_counts <= visit_threshold].index\n",
    "\n",
    "    # Step 3: Filter the dataframe\n",
    "    filtered_df = df[df[\"SUBJECT_ID\"].isin(valid_subjects)].copy()\n",
    "\n",
    "    # Optional: logging\n",
    "    num_removed_patients = df[\"SUBJECT_ID\"].nunique() - filtered_df[\"SUBJECT_ID\"].nunique()\n",
    "    num_removed_rows = len(df) - len(filtered_df)\n",
    "    print(f\"Removed {num_removed_patients} patients and {num_removed_rows} rows exceeding {visit_threshold} visits.\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e669c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process_all():\n",
    "    print('process_med')\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "\n",
    "    print('process_diag')\n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_most_diag(diag_pd)\n",
    "\n",
    "    print('process_pro')\n",
    "    pro_pd = process_procedure()\n",
    "    pro_pd = filter_most_pro(pro_pd)\n",
    "\n",
    "    print('process_ad')\n",
    "    ad_pd = process_admission()\n",
    "    \n",
    "    print('process_lab')\n",
    "    lab_pd = process_lab_test()\n",
    "    lab_pd = filter_most_lab(lab_pd)\n",
    "    \n",
    "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    lab_pd_key = lab_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    ad_pd_key = ad_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    \n",
    "    # filter key\n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(lab_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(ad_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    lab_pd = lab_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    ad_pd = ad_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
    "    lab_pd = lab_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['LAB_TEST'].unique().reset_index()\n",
    "    \n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n",
    "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
    "    lab_pd['LAB_TEST'] = lab_pd['LAB_TEST'].map(lambda x: list(x))\n",
    "    \n",
    "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(lab_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(ad_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    data = data.sort_values(by=['SUBJECT_ID', 'ADMITTIME'])\n",
    "    \n",
    "    # create feature: readmission within 30/90 days\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])\n",
    "    data['READMISSION_1M'] = data.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1) - data['ADMITTIME']\n",
    "    data['READMISSION_3M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=90) else 0)\n",
    "    data['READMISSION_1M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=30) else 0)\n",
    "    \n",
    "    # create feature: diease in next 6/12 months    \n",
    "    data['NEXT_DIAG_6M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                              (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                              (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=180))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_12M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                                   (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                                   (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=365))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_6M'] = data['NEXT_DIAG_6M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    data['NEXT_DIAG_12M'] = data['NEXT_DIAG_12M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    \n",
    "    data.drop(columns=['ADMITTIME'], axis=1, inplace=True)\n",
    "    data = remove_high_visit_patients(data, visit_threshold=8)\n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process_all():\n",
    "    print('process_med')\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "\n",
    "    print('process_diag')\n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_most_diag(diag_pd)\n",
    "\n",
    "    print('process_pro')\n",
    "    pro_pd = process_procedure()\n",
    "    pro_pd = filter_most_pro(pro_pd)\n",
    "\n",
    "    print('process_ad')\n",
    "    ad_pd = process_admission()\n",
    "    \n",
    "    print('process_lab')\n",
    "    lab_pd = process_lab_test()\n",
    "    lab_pd = filter_most_lab(lab_pd)\n",
    "    \n",
    "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    lab_pd_key = lab_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    ad_pd_key = ad_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    \n",
    "    # filter key\n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(lab_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(ad_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    lab_pd = lab_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    ad_pd = ad_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
    "    lab_pd = lab_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['LAB_TEST'].unique().reset_index()\n",
    "    \n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n",
    "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
    "    lab_pd['LAB_TEST'] = lab_pd['LAB_TEST'].map(lambda x: list(x))\n",
    "    \n",
    "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(lab_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(ad_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    data = data.sort_values(by=['SUBJECT_ID', 'ADMITTIME'])\n",
    "    \n",
    "    # filter rows without disease codes\n",
    "    data = data[data['ICD9_CODE'].notnull()]\n",
    "    \n",
    "    # create feature: readmission within 30/90 days\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])\n",
    "    data['READMISSION_1M'] = data.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1) - data['ADMITTIME']\n",
    "    data['READMISSION_3M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=90) else 0)\n",
    "    data['READMISSION_1M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=30) else 0)\n",
    "    \n",
    "    # create feature: diease in next 6/12 months    \n",
    "    data['NEXT_DIAG_6M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                              (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                              (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=180))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_12M'] = data.apply(lambda x: data[(data['SUBJECT_ID'] == x['SUBJECT_ID']) & \n",
    "                                                   (data['ADMITTIME'] > x['ADMITTIME']) & \n",
    "                                                   (data['ADMITTIME'] <= x['ADMITTIME'] + timedelta(days=365))]['ICD9_CODE'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_6M'] = data['NEXT_DIAG_6M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    data['NEXT_DIAG_12M'] = data['NEXT_DIAG_12M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    \n",
    "    data.drop(columns=['ADMITTIME'], axis=1, inplace=True)\n",
    "    data = remove_high_visit_patients(data, visit_threshold=8)\n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f2c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(data):\n",
    "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
    "    print('#clinical events ', len(data))\n",
    "    \n",
    "    diag = data['ICD9_CODE'].values\n",
    "    med = data['NDC'].values\n",
    "    pro = data['PRO_CODE'].values\n",
    "    lab_test = data['LAB_TEST'].values\n",
    "    \n",
    "    unique_diag = set([j for i in diag for j in list(i)])\n",
    "    unique_med = set([j for i in med for j in list(i)])\n",
    "    unique_pro = set([j for i in pro for j in list(i)])\n",
    "    unique_lab = set([j for i in lab_test for j in list(i)])\n",
    "    \n",
    "    print('#diagnosis ', len(unique_diag))\n",
    "    print('#med ', len(unique_med))\n",
    "    print('#procedure', len(unique_pro))\n",
    "    print('#lab', len(unique_lab))\n",
    "    \n",
    "    avg_diag = avg_med = avg_pro = avg_lab = 0\n",
    "    max_diag = max_med = max_pro = max_lab = 0\n",
    "    cnt = max_visit = avg_visit = 0\n",
    "\n",
    "    for subject_id in data['SUBJECT_ID'].unique():\n",
    "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
    "        x, y, z, k = [], [], [], []\n",
    "        visit_cnt = 0\n",
    "        for index, row in item_data.iterrows():\n",
    "            visit_cnt += 1\n",
    "            cnt += 1\n",
    "            x.extend(list(row['ICD9_CODE']))\n",
    "            y.extend(list(row['NDC']))\n",
    "            z.extend(list(row['PRO_CODE']))\n",
    "            k.extend(list(row['LAB_TEST']))\n",
    "        x, y, z, k = set(x), set(y), set(z), set(k)\n",
    "        avg_diag += len(x)\n",
    "        avg_med += len(y)\n",
    "        avg_pro += len(z)\n",
    "        avg_lab += len(k)\n",
    "        avg_visit += visit_cnt\n",
    "        if len(x) > max_diag:\n",
    "            max_diag = len(x)\n",
    "        if len(y) > max_med:\n",
    "            max_med = len(y) \n",
    "        if len(z) > max_pro:\n",
    "            max_pro = len(z)\n",
    "        if len(k) > max_lab:\n",
    "            max_lab = len(k)\n",
    "        if visit_cnt > max_visit:\n",
    "            max_visit = visit_cnt\n",
    "\n",
    "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
    "    print('#avg of medicines ', avg_med/ cnt)\n",
    "    print('#avg of procedures ', avg_pro/ cnt)\n",
    "    print('#avg of lab_test ', avg_lab/ cnt)\n",
    "    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
    "\n",
    "    print('#max of diagnoses ', max_diag)\n",
    "    print('#max of medicines ', max_med)\n",
    "    print('#max of procedures ', max_pro)\n",
    "    print('#max of lab_test ', max_lab)\n",
    "    print('#max of visit ', max_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dac3417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2312/2012914433.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  med_pd = pd.read_csv(med_file, dtype={'ndc':'category'})\n",
      "/var/folders/ml/wn82vzfn1k1_7ymwqp44hm_c0000gn/T/ipykernel_2312/2012914433.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  med_pd.fillna(method='pad', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_diag\n",
      "process_pro\n",
      "process_ad\n",
      "process_lab\n",
      "Removed 65 patients and 647 rows exceeding 8 visits.\n"
     ]
    }
   ],
   "source": [
    "data = process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08947e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#patients  (60709,)\n",
      "#clinical events  84495\n",
      "#diagnosis  1983\n",
      "#med  140\n",
      "#procedure 801\n",
      "#lab 1281\n",
      "#avg of diagnoses  10.262867625303272\n",
      "#avg of medicines  2.976105094976034\n",
      "#avg of procedures  2.867246582638026\n",
      "#avg of lab_test  15.083472394816262\n",
      "#avg of vists  1.3918035217183613\n",
      "#max of diagnoses  101\n",
      "#max of medicines  30\n",
      "#max of procedures  41\n",
      "#max of lab_test  139\n",
      "#max of visit  8\n"
     ]
    }
   ],
   "source": [
    "statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "962ac375",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MIMIC-IV-processed/mimic.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa3c13",
   "metadata": {},
   "source": [
    "# Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60bb543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data = pickle.load(open(\"./MIMIC-IV-processed/mimic.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "431d9e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'NDC', 'PRO_CODE', 'LAB_TEST',\n",
       "       'ADMISSION_TYPE', 'STAY_DAYS', 'GENDER', 'AGE', 'DEATH', 'READMISSION',\n",
       "       'READMISSION_1M', 'READMISSION_3M', 'NEXT_DIAG_6M', 'NEXT_DIAG_12M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaba8154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60709"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_data[\"SUBJECT_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4c5202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4604\n",
      "9667\n",
      "11197\n",
      "3577\n"
     ]
    }
   ],
   "source": [
    "# get the patient id with the labels\n",
    "pat_readmission = set(mimic_data[mimic_data[\"READMISSION_1M\"] == 1][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_readmission))\n",
    "pat_nextdiag_6m = set(mimic_data[mimic_data[\"NEXT_DIAG_6M\"].notna()][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_nextdiag_6m))\n",
    "# note that here we extract at the patient level, not the encounter level\n",
    "# so patients with at least one encounter with a next diagnosis in 6 / 12 months will still have\n",
    "# some encounters without a next diagnosis. This part is addressed in the HBERTFinetuneEHRDataset.\n",
    "pat_nextdiag_12m = set(mimic_data[mimic_data[\"NEXT_DIAG_12M\"].notna()][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_nextdiag_12m))\n",
    "pat_death = set(mimic_data[mimic_data[\"DEATH\"]][\"SUBJECT_ID\"].values.tolist())\n",
    "print(len(pat_death))\n",
    "pat_all_label = list(pat_readmission | pat_nextdiag_6m | pat_nextdiag_12m | pat_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daa642df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_all = mimic_data[\"SUBJECT_ID\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db3dd443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42496 18213\n"
     ]
    }
   ],
   "source": [
    "n_pretrain_patient = int(len(pat_all) * 0.7)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pretrain_patient = np.random.choice(list(set(pat_all) - set(pat_all_label)), n_pretrain_patient, replace=False).tolist()\n",
    "downstream_patient = list(set(pat_all) - set(pretrain_patient))\n",
    "print(len(pretrain_patient), len(downstream_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25390241",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(pretrain_patient))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81de4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream task: PLOS, death prediction, readmission\n",
    "train_ratio, val_ratio = 0.2, 0.4\n",
    "n_finetune_pat, n_val_pat = int(len(downstream_patient) * train_ratio), int(len(downstream_patient) * val_ratio)\n",
    "downstream_patient = sorted(downstream_patient) \n",
    "np.random.seed(RANDOM_SEED + 1)\n",
    "np.random.shuffle(downstream_patient)\n",
    "finetune_pat, val_pat, test_pat = downstream_patient[:n_finetune_pat], \\\n",
    "                                    downstream_patient[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    downstream_patient[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b351ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream task: next diagnosis prediction\n",
    "# only use the patient that has multiple visits\n",
    "# 6m\n",
    "train_ratio, val_ratio = 0.4, 0.3\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_6m) * train_ratio), int(len(pat_nextdiag_6m) * val_ratio)\n",
    "pat_nextdiag_6m = list(pat_nextdiag_6m)\n",
    "pat_nextdiag_6m = sorted(list(pat_nextdiag_6m))  # 先排序\n",
    "np.random.seed(RANDOM_SEED + 2)  # 设置种子\n",
    "np.random.shuffle(pat_nextdiag_6m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_6m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset6m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]\n",
    "# 12m\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_12m) * train_ratio), int(len(pat_nextdiag_12m) * val_ratio)\n",
    "pat_nextdiag_12m = list(pat_nextdiag_12m)\n",
    "pat_nextdiag_12m = sorted(list(pat_nextdiag_12m))  # 先排序\n",
    "np.random.seed(RANDOM_SEED + 3)  # 设置种子\n",
    "np.random.shuffle(pat_nextdiag_12m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_12m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(finetune_pat))]\n",
    "val_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(val_pat))]\n",
    "test_dataset12m = mimic_data[mimic_data[\"SUBJECT_ID\"].isin(set(test_pat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99e983bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./MIMIC-IV-processed/mimic_pretrain.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(pretrain_dataset, outfile)\n",
    "with open(\"./MIMIC-IV-processed/mimic_downstream.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset, val_dataset, test_dataset], outfile)\n",
    "with open(\"./MIMIC-IV-processed/mimic_nextdiag_6m.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset6m, val_dataset6m, test_dataset6m], outfile)\n",
    "with open(\"./MIMIC-IV-processed/mimic_nextdiag_12m.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump([finetune_dataset12m, val_dataset12m, test_dataset12m], outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
